{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7yCFcPcy7Qt",
        "outputId": "9b40fc05-627e-4678-e0b6-4c47af4781b5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Time travel‚Äîan enigmatic concept‚Äîintrigues scientists, philosophers, & dreamers alike! üöÄüï∞Ô∏è Imagine zipping through spacetime, \\n\\\n",
        "witnessing the birth of galaxies or altering history (but wait‚Ä¶ don't mess with the past! ‚ö†Ô∏è‚è≥).\""
      ],
      "metadata": {
        "id": "lqkVeXaiy8wB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sentence tokenization\n",
        "Splits text into sentences instead of words.\n"
      ],
      "metadata": {
        "id": "31QpbQzSy9F1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qET_wuF4zx3j",
        "outputId": "0be5ba2b-5091-41e5-e714-770868d3051a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Time travel‚Äîan enigmatic concept‚Äîintrigues scientists, philosophers, & dreamers alike!',\n",
              " \"üöÄüï∞Ô∏è Imagine zipping through spacetime, \\nwitnessing the birth of galaxies or altering history (but wait‚Ä¶ don't mess with the past!\",\n",
              " '‚ö†Ô∏è‚è≥).']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word tokenization\n",
        "Splits text into words."
      ],
      "metadata": {
        "id": "uMJyRENKzDKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83M-jFoMz4Tj",
        "outputId": "44d301e1-b40e-4c2d-f252-5ed8e4d80a06"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Time',\n",
              " 'travel‚Äîan',\n",
              " 'enigmatic',\n",
              " 'concept‚Äîintrigues',\n",
              " 'scientists',\n",
              " ',',\n",
              " 'philosophers',\n",
              " ',',\n",
              " '&',\n",
              " 'dreamers',\n",
              " 'alike',\n",
              " '!',\n",
              " 'üöÄüï∞Ô∏è',\n",
              " 'Imagine',\n",
              " 'zipping',\n",
              " 'through',\n",
              " 'spacetime',\n",
              " ',',\n",
              " 'witnessing',\n",
              " 'the',\n",
              " 'birth',\n",
              " 'of',\n",
              " 'galaxies',\n",
              " 'or',\n",
              " 'altering',\n",
              " 'history',\n",
              " '(',\n",
              " 'but',\n",
              " 'wait‚Ä¶',\n",
              " 'do',\n",
              " \"n't\",\n",
              " 'mess',\n",
              " 'with',\n",
              " 'the',\n",
              " 'past',\n",
              " '!',\n",
              " '‚ö†Ô∏è‚è≥',\n",
              " ')',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Punctuation-based Tokenizer\n",
        "Uses punctuation marks as delimiters to split words."
      ],
      "metadata": {
        "id": "4s-P9lYyz00K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "tokens = re.findall(r\"[\\w']+|[.,!?;]\", text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACDm3MZD6daF",
        "outputId": "263d87d9-8741-4eb8-da6f-ba1d19c64824"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Time', 'travel', 'an', 'enigmatic', 'concept', 'intrigues', 'scientists', ',', 'philosophers', ',', 'dreamers', 'alike', '!', 'Imagine', 'zipping', 'through', 'spacetime', ',', 'witnessing', 'the', 'birth', 'of', 'galaxies', 'or', 'altering', 'history', 'but', 'wait', \"don't\", 'mess', 'with', 'the', 'past', '!', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TreeBankword Tokenizer\n",
        "\n",
        "The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank.\n",
        "\n",
        "This tokenizer performs the following steps:\n",
        "\n",
        "split standard contractions, e.g. don't -> do n't and they'll -> they 'll\n",
        "\n",
        "treat most punctuation characters as separate tokens\n",
        "\n",
        "split off commas and single quotes, when followed by whitespace\n",
        "\n",
        "separate periods that appear at the end of line"
      ],
      "metadata": {
        "id": "bR_kglc76eWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "s = \"They'll save and invest more.\"\n",
        "s1 = \"hi, my name can't hello,\"\n",
        "print(TreebankWordTokenizer().tokenize(s))\n",
        "print(TreebankWordTokenizer().tokenize(s1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzhmhX1d03Eg",
        "outputId": "ab04c477-0152-4f7a-9b0a-46a23a2d35d2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['They', \"'ll\", 'save', 'and', 'invest', 'more', '.']\n",
            "['hi', ',', 'my', 'name', 'ca', \"n't\", 'hello', ',']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tweet Tokenizer\n",
        "\n",
        "With the help of NLTK nltk.TweetTokenizer() method, we are able to convert the stream of words into small  tokens so that we can analyse the audio stream with the help of nltk.TweetTokenizer() method."
      ],
      "metadata": {
        "id": "QzxhQ1-z7DNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import TweetTokenizer() method from nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "# Create a reference variable for Class TweetTokenizer\n",
        "tk = TweetTokenizer()\n",
        "print(tk.tokenize(text) )\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qoH5zcr5PHw",
        "outputId": "89e0b559-db26-4720-e114-4230f33b4ce7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Time', 'travel', '‚Äî', 'an', 'enigmatic', 'concept', '‚Äî', 'intrigues', 'scientists', ',', 'philosophers', ',', '&', 'dreamers', 'alike', '!', 'üöÄ', 'üï∞', 'Ô∏è', 'Imagine', 'zipping', 'through', 'spacetime', ',', 'witnessing', 'the', 'birth', 'of', 'galaxies', 'or', 'altering', 'history', '(', 'but', 'wait', '‚Ä¶', \"don't\", 'mess', 'with', 'the', 'past', '!', '‚ö†', 'Ô∏è', '‚è≥', ')', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multi-Word Expression Tokenizer\n",
        "\n",
        " Identifies multi-word expressions as single tokens."
      ],
      "metadata": {
        "id": "E4MgZSvW91cO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "tokenizer = MWETokenizer([('New', 'York'), ('machine', 'learning')])\n",
        "text = \"I live in New York and love machine learning .\"\n",
        "tokens = tokenizer.tokenize(text.split())\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTahrMVn7uiN",
        "outputId": "96b31b13-2987-4e9f-b9fd-13e086c1de73"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'live', 'in', 'New_York', 'and', 'love', 'machine_learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Blob\n",
        "\n",
        "Uses the TextBlob library for simple word tokenization."
      ],
      "metadata": {
        "id": "52tioXctpf-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n"
      ],
      "metadata": {
        "id": "a4TrddQHpKrO"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a TextBlob object\n",
        "blob_object = TextBlob(text)\n",
        "\n",
        "# tokenize paragraph into words.\n",
        "print(\" Word Tokenize :\\n\", blob_object.words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79S0JKj5pLos",
        "outputId": "6721dc4f-d155-470a-99cd-ea9ab514ad2f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Word Tokenize :\n",
            " ['I', 'live', 'in', 'New', 'York', 'and', 'love', 'machine', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# spaCy Tokenizer\n",
        "\n",
        "Tokenizer from the spaCy library, optimized for speed and accuracy."
      ],
      "metadata": {
        "id": "T8ejzsOMqKt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First we need to import spacy\n",
        "import spacy\n",
        "\n",
        "# Creating blank language object then\n",
        "# tokenizing words of the sentence\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXj54YSTpitk",
        "outputId": "0b131060-f4aa-40fc-d6ea-f4b4baa2163f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "live\n",
            "in\n",
            "New\n",
            "York\n",
            "and\n",
            "love\n",
            "machine\n",
            "learning\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gensim word tokenizer\n",
        "\n",
        " Uses gensim.utils.simple_preprocess() for efficient tokenization."
      ],
      "metadata": {
        "id": "okxUzIK1rOWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import tokenize\n",
        "\n",
        "tokens = list(tokenize(text))\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94iN7nuCqkjH",
        "outputId": "3a9f6e11-1538-48ab-ca76-c93720ebf8ed"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'live', 'in', 'New', 'York', 'and', 'love', 'machine', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization with Keras\n",
        "\n",
        "Keras provides tokenization using Tokenizer() for deep learning preprocessing."
      ],
      "metadata": {
        "id": "dir61CqSr4Y1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = text_to_word_sequence(text)\n",
        "\n",
        "print(\"Tokenized Words:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A79nsVtpuMLm",
        "outputId": "18d6d456-5e9f-452c-e116-738c73f89ae4"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Words: ['i', 'live', 'in', 'new', 'york', 'and', 'love', 'machine', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eftd9Luxw8BA"
      },
      "execution_count": 46,
      "outputs": []
    }
  ]
}